{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaanJam/faster-whisper-youtube/blob/main/%E2%80%9Cfaster_whisper_youtube_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with Faster Whisper**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/lewangdev/whisper-youtube/blob/main/faster_whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/lewangdev/faster_whisper_youtube)](https://github.com/lewangdev/faster_whisper_youtube)\n",
        "\n",
        "\n",
        "[faster-whisper](https://github.com/guillaumekln/faster-whisper) is a reimplementation of OpenAI's Whisper model using CTranslate2, which is a fast inference engine for Transformer models.\n",
        "\n",
        "This implementation is up to 4 times faster than openai/whisper for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Faster Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check GPU type** üïµÔ∏è\n"
      ],
      "metadata": {
        "id": "n3nD9M512ZxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QshUbLqpX7L4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install libraries** üèóÔ∏è"
      ],
      "metadata": {
        "id": "3yig3V8p2aLY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown This cell will take a little while to download several libraries, including Faster Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install faster-whisper\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "from faster_whisper import WhisperModel\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optional:** Save data in Google Drive üíæ\n"
      ],
      "metadata": {
        "id": "s8FRU2nL3Te_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2770f8-baf0-4ee4-9319-525eef19a4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Faster Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model selection** üß†\n"
      ],
      "metadata": {
        "id": "koVQDUXV3fEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~0.8 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1.0 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~1.4 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~2.7 GB     |      ~2x       |\n",
        "#@markdown | large-v1  |   1550 M   |        N/A         |      `large-v1`       |    ~4.3 GB     |       1x       |\n",
        "#@markdown | large-v2  |   1550 M   |        N/A         |      `large-v2`       |    ~4.3 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "model_size = 'large-v2' #@param ['tiny', 'tiny.en', 'base', 'base.en', 'small', 'small.en', 'medium', 'medium.en', 'large-v1', 'large-v2']\n",
        "device_type = \"cuda\" #@param {type:\"string\"} ['cuda', 'cpu']\n",
        "compute_type = \"float16\" #@param {type:\"string\"} ['float16', 'int8_float16', 'int8']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "model = WhisperModel(model_size, device=device_type, compute_type=compute_type)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TMhrSq_GZ6kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Video selection** üì∫\n"
      ],
      "metadata": {
        "id": "cWEK0OsL3o7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "import json\n",
        "import os\n",
        "Type = \"Google Drive\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/OUPBCCpQ-ro\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"mv/\\u5982\\u4F55\\u79AE\\u4F5B\\u53CA\\u5176\\u5BC6\\u7FA9 \\u6210\\u89C0\\u6CD5\\u5E2B.mp4 - \\u4E0D\\u5982\\u5403\\u8336\\u53BB.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(title)s.%(ext)s',\n",
        "        # ‚ÑπÔ∏è See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'm4a',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        list_video_info = [ydl.extract_info(URL, download=True)]\n",
        "        # error_code = ydl.download([URL])\n",
        "\n",
        "    for video_info in list_video_info:\n",
        "        if 'entries' in video_info:\n",
        "            list_video_entries = video_info['entries']\n",
        "            for entry in list_video_entries:\n",
        "                video_path_local_list.append(Path(f\"{entry['title']}.m4a\"))\n",
        "        else:\n",
        "            video_path_local_list.append(Path(f\"{video_info['title']}.m4a\"))\n",
        "\n",
        "elif Type == \"Google Drive\":\n",
        "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "            if video_path_drive.is_file():\n",
        "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "            elif video_path_drive.is_dir():\n",
        "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "            else:\n",
        "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "            shutil.copy(video_path_drive, video_path_local)\n",
        "            video_path_local_list.append(video_path_local)\n",
        "    elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "    else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "else:\n",
        "    raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "# print(list_video_info[0]['entries'][0])\n",
        "for video_path_local in video_path_local_list:\n",
        "    print(video_path_local)\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "outputId": "ec0b3227-fb51-4076-ddc4-a4f60265eac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**/content/drive/My Drive/mv/Â¶Ç‰ΩïÁ¶Æ‰ΩõÂèäÂÖ∂ÂØÜÁæ© ÊàêËßÄÊ≥ïÂ∏´.mp4 - ‰∏çÂ¶ÇÂêÉËå∂Âéª.mp4 selected for transcription.**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Â¶Ç‰ΩïÁ¶Æ‰ΩõÂèäÂÖ∂ÂØÜÁæ© ÊàêËßÄÊ≥ïÂ∏´.mp4 - ‰∏çÂ¶ÇÂêÉËå∂Âéª.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run the model** üöÄ\n"
      ],
      "metadata": {
        "id": "i0sCAwd23xrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and vary based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ‚öôÔ∏è\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown #### Language\n",
        "language = \"zh\" #@param [\"auto\", \"en\", \"zh\", \"ja\", \"fr\", \"de\"] {allow-input: true}\n",
        "#@markdown #### initial prompt\n",
        "initial_prompt = \"Please do not translate, only word-for-word transcription is allowed.  Here are some English words you may need: Cindy. And Chinese words: \\u793C\\u4F5B\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Word-level timestamps\n",
        "word_level_timestamps = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### VAD filter\n",
        "vad_filter = True #@param {type:\"boolean\"}\n",
        "vad_filter_min_silence_duration_ms = 50 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Output(Default is srt, txt if `text_only` be checked )\n",
        "text_only = False #@param {type:\"boolean\"}\n",
        "\n",
        "def seconds_to_time_format(s):\n",
        "    # Convert seconds to hours, minutes, seconds, and milliseconds\n",
        "    hours = s // 3600\n",
        "    s %= 3600\n",
        "    minutes = s // 60\n",
        "    s %= 60\n",
        "    seconds = s // 1\n",
        "    milliseconds = round((s % 1) * 1000)\n",
        "\n",
        "    # Return the formatted string\n",
        "    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},{int(milliseconds):03d}\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    print(f\"Processing video: {video_path_local}\")  # ËæìÂá∫ÂΩìÂâç video_path_local\n",
        "    segments, info = model.transcribe(str(video_path_local), beam_size=5,\n",
        "                                      language=None if language == \"auto\" else language,\n",
        "                                      initial_prompt=initial_prompt,\n",
        "                                      word_timestamps=word_level_timestamps,\n",
        "                                      vad_filter=vad_filter,\n",
        "                                      vad_parameters=dict(min_silence_duration_ms=vad_filter_min_silence_duration_ms))\n",
        "\n",
        "    display(Markdown(f\"Detected language '{info.language}' with probability {info.language_probability}\"))\n",
        "\n",
        "    ext_name = '.txt' if text_only else \".srt\"\n",
        "    transcript_file_name = video_path_local.stem + ext_name\n",
        "    sentence_idx = 1\n",
        "    with open(transcript_file_name, 'w') as f:\n",
        "        for segment in segments:\n",
        "            if word_level_timestamps:\n",
        "                for word in segment.words:\n",
        "                    ts_start = seconds_to_time_format(word.start)\n",
        "                    ts_end = seconds_to_time_format(word.end)\n",
        "                    print(f\"[{ts_start} --> {ts_end}] {word.word}\")\n",
        "                    if not text_only:\n",
        "                        f.write(f\"{sentence_idx}\\n\")\n",
        "                        f.write(f\"{ts_start} --> {ts_end}\\n\")\n",
        "                        f.write(f\"{word.word}\\n\\n\")\n",
        "                    else:\n",
        "                        f.write(f\"{word.word}\")\n",
        "                    f.write(\"\\n\")\n",
        "                    sentence_idx = sentence_idx + 1\n",
        "            else:\n",
        "                ts_start = seconds_to_time_format(segment.start)\n",
        "                ts_end = seconds_to_time_format(segment.end)\n",
        "                print(f\"[{ts_start} --> {ts_end}] {segment.text}\")\n",
        "                if not text_only:\n",
        "                    f.write(f\"{sentence_idx}\\n\")\n",
        "                    f.write(f\"{ts_start} --> {ts_end}\\n\")\n",
        "                    f.write(f\"{segment.text.strip()}\\n\\n\")\n",
        "                else:\n",
        "                    f.write(f\"{segment.text.strip()}\\n\")\n",
        "                sentence_idx = sentence_idx + 1\n",
        "\n",
        "    try:\n",
        "        shutil.copy(video_path_local.parent / transcript_file_name, drive_whisper_path / transcript_file_name)\n",
        "        display(Markdown(f\"**Transcript file created: {drive_whisper_path / transcript_file_name}**\"))\n",
        "    except:\n",
        "        display(Markdown(f\"**Transcript file created: {video_path_local.parent / transcript_file_name}**\"))\n"
      ],
      "metadata": {
        "id": "Ad6n1m4deAHp",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf *.*"
      ],
      "metadata": {
        "id": "RVnTvb-0yZLi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n3nD9M512ZxI",
        "3yig3V8p2aLY",
        "s8FRU2nL3Te_",
        "koVQDUXV3fEQ",
        "cWEK0OsL3o7h",
        "i0sCAwd23xrV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}